{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing & Feature Engineering"
      ],
      "metadata": {
        "id": "9aeDt-OEG7Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "LaLdqaalG_Zi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "ASTHMA_DATA_PATH = \"/content/drive/My Drive/Big Data Project/Data/Raw/asthma_dataset/Asthma Prevalance_Data_2020_2023.csv\"\n",
        "POLLUTANT_DATA_PATH = \"/content/drive/My Drive/Big Data Project/Data/Processed/all_pollutants_merged_inner.csv\"\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/Big Data Project/Data/Processed/Semi_Supervised_Learning_Model.csv\" # New output path\n",
        "\n",
        "# Define pollutants to process\n",
        "POLLUTANTS = ['PM25', 'O3', 'NO2', 'SO2', 'CO']\n",
        "\n",
        "# Meaningful thresholds values relevant to health guidelines\n",
        "THRESHOLDS = {\n",
        "    'PM25': 9.0,  # EPA AQI \"Moderate\" 24-hour PM2.5 (ug/m3)\n",
        "    'O3': 0.070,   # EPA AQI \"Moderate\" 8-hour Ozone (ppm)\n",
        "    'NO2': 53,    # EPA 1-hour NO2 standard (ppb)\n",
        "    'SO2': 0.1,     # EPA 1-hour SO2 standard (ppb)\n",
        "    'CO': 0.35       # EPA 8-hour CO standard (ppm)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfpDSskCG_Wr",
        "outputId": "049829ad-7434-44ba-c878-d8952d168fdb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load Data ---\n",
        "print(\"--- 1. Loading Data ---\")\n",
        "try:\n",
        "    df_asthma = pd.read_csv(ASTHMA_DATA_PATH)\n",
        "    print(f\"Asthma data loaded: {df_asthma.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Asthma data file not found at {ASTHMA_DATA_PATH}\")\n",
        "    df_asthma = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    df_pollutant_raw = pd.read_csv(POLLUTANT_DATA_PATH, low_memory=False)\n",
        "    print(f\"Pollutant data loaded: {df_pollutant_raw.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Pollutant data file not found at {POLLUTANT_DATA_PATH}\")\n",
        "    df_pollutant_raw = pd.DataFrame()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esC_DkwFG_UE",
        "outputId": "92039d33-c10c-41ed-8eb2-8be7f3a02856"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Loading Data ---\n",
            "Asthma data loaded: (224, 4)\n",
            "Pollutant data loaded: (98351, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Prepare Pollutant Data ---\n",
        "if not df_pollutant_raw.empty:\n",
        "    print(\"\\n--- 2. Preparing Pollutant Data ---\")\n",
        "    df_pollutant = df_pollutant_raw.copy()\n",
        "\n",
        "    # Convert 'Date Local' to datetime and extract Year/Month\n",
        "    if 'Date Local' in df_pollutant.columns:\n",
        "        df_pollutant['Date Local'] = pd.to_datetime(df_pollutant['Date Local'], errors='coerce')\n",
        "        df_pollutant.dropna(subset=['Date Local'], inplace=True)\n",
        "        df_pollutant['Year'] = df_pollutant['Date Local'].dt.year\n",
        "        df_pollutant['Month'] = df_pollutant['Date Local'].dt.month\n",
        "    else:\n",
        "        print(\"ERROR: 'Date Local' column not found.\")\n",
        "        df_pollutant = pd.DataFrame()\n",
        "\n",
        "    # Standardize 'County Name' and 'State Name'\n",
        "    if 'County Name' in df_pollutant.columns:\n",
        "        df_pollutant['County Name'] = df_pollutant['County Name'].astype(str).str.strip().str.lower()\n",
        "    else:\n",
        "        print(\"ERROR: 'County Name' column not found.\")\n",
        "        df_pollutant = pd.DataFrame()\n",
        "\n",
        "    if 'State Name' in df_pollutant.columns:\n",
        "        df_pollutant['State Name'] = df_pollutant['State Name'].astype(str).str.strip().str.lower()\n",
        "    else:\n",
        "        print(\"ERROR: 'State Name' column not found.\")\n",
        "        df_pollutant = pd.DataFrame()\n",
        "\n",
        "    # Ensure pollutants are numeric and floor to 0\n",
        "    POLLUTANTS_TO_FLOOR = ['PM25', 'O3', 'NO2', 'SO2', 'CO']\n",
        "    print(\"--- Flooring Pollutant Levels to 0 ---\")\n",
        "    for p in POLLUTANTS_TO_FLOOR:\n",
        "        if p in df_pollutant.columns:\n",
        "            df_pollutant[p] = pd.to_numeric(df_pollutant[p], errors='coerce')\n",
        "            neg_count_before = (df_pollutant[p] < 0).sum()\n",
        "            df_pollutant[p] = df_pollutant[p].clip(lower=0)\n",
        "            print(f\"Processed '{p}': Found and floored {neg_count_before} negative value(s).\")\n",
        "        else:\n",
        "            print(f\"Warning: Pollutant column '{p}' not found.\")\n",
        "else:\n",
        "     df_pollutant = pd.DataFrame() # Ensure it's empty if loading failed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbQIUGDuG_RQ",
        "outputId": "53bf06c9-1b9f-4c08-bf5d-c65bd0131f25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Preparing Pollutant Data ---\n",
            "--- Flooring Pollutant Levels to 0 ---\n",
            "Processed 'PM25': Found and floored 287 negative value(s).\n",
            "Processed 'O3': Found and floored 1 negative value(s).\n",
            "Processed 'NO2': Found and floored 61 negative value(s).\n",
            "Processed 'SO2': Found and floored 6056 negative value(s).\n",
            "Processed 'CO': Found and floored 1348 negative value(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Define Seasons ---\n",
        "if not df_pollutant.empty and 'Month' in df_pollutant.columns:\n",
        "    def get_season(month):\n",
        "        if month in [12, 1, 2]: return 'Winter'\n",
        "        elif month in [3, 4, 5]: return 'Spring'\n",
        "        elif month in [6, 7, 8]: return 'Summer'\n",
        "        elif month in [9, 10, 11]: return 'Fall'\n",
        "        return None\n",
        "    df_pollutant['Season'] = df_pollutant['Month'].apply(get_season)"
      ],
      "metadata": {
        "id": "1DmuSweBHJcg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Feature Engineering (For ALL counties in pollutant data) ---\n",
        "df_engineered_features = pd.DataFrame()\n",
        "if not df_pollutant.empty and 'County Name' in df_pollutant.columns and 'State Name' in df_pollutant.columns and 'Year' in df_pollutant.columns:\n",
        "    print(\"\\n--- 4. Engineering Features ---\")\n",
        "    # Include State Name in groupby\n",
        "    grouped_annual = df_pollutant.groupby(['County Name', 'State Name', 'Year'])\n",
        "\n",
        "    aggs = {}\n",
        "    if 'Latitude' in df_pollutant.columns: aggs['Latitude'] = 'mean'\n",
        "    if 'Longitude' in df_pollutant.columns: aggs['Longitude'] = 'mean'\n",
        "    for p in POLLUTANTS:\n",
        "        if p in df_pollutant.columns:\n",
        "            aggs[p] = ['mean', 'max', 'std', lambda x: x.quantile(0.75) - x.quantile(0.25)]\n",
        "\n",
        "    annual_stats = grouped_annual.agg(aggs)\n",
        "    annual_stats.columns = ['_'.join(col).strip('_') for col in annual_stats.columns.values]\n",
        "    rename_dict = {'Latitude_mean': 'Latitude', 'Longitude_mean': 'Longitude'}\n",
        "    for p in POLLUTANTS:\n",
        "        if p in df_pollutant.columns:\n",
        "            rename_dict[f'{p}_mean'] = f'{p}_Annual_Mean'\n",
        "            rename_dict[f'{p}_max'] = f'{p}_Annual_Max'\n",
        "            rename_dict[f'{p}_std'] = f'{p}_Annual_StdDev'\n",
        "            lambda_col_name = next((col for col in annual_stats.columns if f'{p}_<lambda' in col), None)\n",
        "            if lambda_col_name: rename_dict[lambda_col_name] = f'{p}_Annual_IQR'\n",
        "    annual_stats = annual_stats.rename(columns=rename_dict)\n",
        "\n",
        "    days_threshold_list = []\n",
        "    for p in POLLUTANTS:\n",
        "        if p in THRESHOLDS and p in df_pollutant.columns:\n",
        "            if p == 'O3': days = grouped_annual[p].apply(lambda x: (x < THRESHOLDS[p]).sum()).rename(f'{p}_Days_Below_Threshold')\n",
        "            else: days = grouped_annual[p].apply(lambda x: (x > THRESHOLDS[p]).sum()).rename(f'{p}_Days_Above_Threshold')\n",
        "            days_threshold_list.append(days)\n",
        "    if days_threshold_list:\n",
        "        days_threshold_df = pd.concat(days_threshold_list, axis=1)\n",
        "        annual_stats = annual_stats.merge(days_threshold_df, on=['County Name', 'State Name', 'Year'], how='left')\n",
        "\n",
        "    seasonal_stats_pivot = pd.DataFrame()\n",
        "    if 'Season' in df_pollutant.columns:\n",
        "        grouped_seasonal = df_pollutant.groupby(['County Name', 'State Name', 'Year', 'Season'])\n",
        "        seasonal_means_list = []\n",
        "        for p in POLLUTANTS:\n",
        "            if p in df_pollutant.columns:\n",
        "                seasonal_means_list.append(grouped_seasonal[p].mean().rename(f'{p}_Seasonal_Avg'))\n",
        "        if seasonal_means_list:\n",
        "            seasonal_stats_raw = pd.concat(seasonal_means_list, axis=1)\n",
        "            seasonal_stats_pivot = seasonal_stats_raw.unstack(level='Season')\n",
        "            seasonal_stats_pivot.columns = ['_'.join(col).strip() for col in seasonal_stats_pivot.columns.values]\n",
        "\n",
        "    # Combine all engineered features\n",
        "    df_engineered_features = annual_stats.reset_index()\n",
        "    if not seasonal_stats_pivot.empty:\n",
        "         df_engineered_features = pd.merge(df_engineered_features, seasonal_stats_pivot.reset_index(),\n",
        "                                          on=['County Name', 'State Name', 'Year'], how='outer')\n",
        "    print(f\"Combined engineered features shape: {df_engineered_features.shape}\")\n",
        "else:\n",
        "    print(\"Could not perform feature engineering - check pollutant data and columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMf9R-KZHJZY",
        "outputId": "7f1e74b0-b94a-4ce2-ed9f-fa2a2c1e0766"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 4. Engineering Features ---\n",
            "Combined engineered features shape: (341, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Prepare Asthma Data & Create Hotspot Label ---\n",
        "if not df_asthma.empty:\n",
        "    print(\"\\n--- 5. Preparing Asthma Data & Hotspot Labels ---\")\n",
        "    # Standardize 'County Name' and 'State Name'\n",
        "    df_asthma['County Name'] = df_asthma['County Name'].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Calculate Hotspot label *before* merge\n",
        "    target_col = 'Age-adjusted rate per 10,000'\n",
        "    if target_col in df_asthma.columns:\n",
        "        # Create Hotspot only where target exists\n",
        "        df_asthma['Hotspot'] = df_asthma.groupby('Year')[target_col]\\\n",
        "                                  .transform(lambda x: (x > x.median()).astype(float)) # Use float for NaN\n",
        "        # Set Hotspot to NaN where target is NaN\n",
        "        df_asthma.loc[df_asthma[target_col].isna(), 'Hotspot'] = np.nan\n",
        "        print(\"Hotspot labels created.\")\n",
        "    else:\n",
        "        print(f\"Warning: Target column '{target_col}' not found in asthma data. Cannot create Hotspot labels.\")\n",
        "        df_asthma['Hotspot'] = np.nan # Ensure column exists but is empty\n",
        "\n",
        "    # Select only relevant columns for merging\n",
        "    asthma_cols_to_merge = [\n",
        "        'County Name', 'Year',\n",
        "        'Age-adjusted rate per 10,000', 'Number of cases', 'Hotspot'\n",
        "    ]\n",
        "    # Ensure all selected columns actually exist\n",
        "    asthma_cols_to_merge = [col for col in asthma_cols_to_merge if col in df_asthma.columns]\n",
        "    df_asthma_subset = df_asthma[asthma_cols_to_merge].copy()\n",
        "    # Drop duplicates in asthma data before merge to avoid issues\n",
        "    df_asthma_subset.drop_duplicates(subset=['County Name', 'Year'], inplace=True)\n",
        "\n",
        "else:\n",
        "    df_asthma_subset = pd.DataFrame()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-eRzmiFHJWS",
        "outputId": "f296fc9f-2b68-478b-f610-c2afb67b8d17"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 5. Preparing Asthma Data & Hotspot Labels ---\n",
            "Hotspot labels created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 6. Merge ALL Features with Asthma Data (Left on Features) ---\n",
        "if not df_engineered_features.empty:\n",
        "    print(\"\\n--- 6. Merging All Data ---\")\n",
        "    if not df_asthma_subset.empty:\n",
        "        # Perform a LEFT merge to keep ALL counties and add asthma data where it matches\n",
        "        df_final = pd.merge(df_engineered_features, df_asthma_subset,\n",
        "                            on=['County Name', 'Year'],\n",
        "                            how='left')\n",
        "        # print(\"Merged engineered features with asthma data.\") # Removed\n",
        "    else:\n",
        "        print(\"Asthma data is empty, final data will only contain engineered features.\")\n",
        "        df_final = df_engineered_features.copy()\n",
        "\n",
        "    # print(f\"Final merged data shape (before removing columns): {df_final.shape}\") # Removed\n",
        "\n",
        "    # --- 7. Modify Columns, Reorder, and Save ---\n",
        "    print(f\"\\n--- 7. Modifying Columns, Reordering, and Preparing for Save ---\")\n",
        "\n",
        "    if not df_final.empty:\n",
        "        # --- Remove Specified Columns ---\n",
        "        cols_to_remove = ['Number of cases', 'Age-adjusted rate per 10,000']\n",
        "        for col in cols_to_remove:\n",
        "            if col in df_final.columns:\n",
        "                df_final = df_final.drop(columns=[col])\n",
        "                # print(f\"Removed '{col}' column. New shape: {df_final.shape}\") # Removed\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col}' not found, nothing to remove.\")\n",
        "\n",
        "        # --- Move 'Hotspot' column before 'PM25_Annual_Mean' ---\n",
        "        col_to_move = 'Hotspot'\n",
        "        insertion_point_col = 'PM25_Annual_Mean'\n",
        "\n",
        "        if col_to_move in df_final.columns and insertion_point_col in df_final.columns:\n",
        "            current_cols = df_final.columns.tolist()\n",
        "            current_cols.remove(col_to_move)\n",
        "            idx_insertion = current_cols.index(insertion_point_col)\n",
        "            current_cols.insert(idx_insertion, col_to_move)\n",
        "            df_final = df_final[current_cols]\n",
        "            # print(f\"Moved '{col_to_move}' column before '{insertion_point_col}'.\") # Removed\n",
        "        elif col_to_move not in df_final.columns:\n",
        "            print(f\"Warning: Column '{col_to_move}' not found. Cannot move it.\")\n",
        "        elif insertion_point_col not in df_final.columns:\n",
        "            print(f\"Warning: Insertion point column '{insertion_point_col}' not found. Cannot move '{col_to_move}'.\")\n",
        "\n",
        "        # --- Print Head of Final DataFrame ---\n",
        "        print(\"\\n--- Head of Final Processed DataFrame ---\")\n",
        "        print(df_final.head())\n",
        "        print(f\"\\nShape of final DataFrame: {df_final.shape}\")\n",
        "        print(f\"Columns in final DataFrame: {df_final.columns.tolist()}\")\n",
        "\n",
        "\n",
        "        # Save the modified DataFrame\n",
        "        # Ensure OUTPUT_PATH is defined from your main script configuration\n",
        "        df_final.to_csv(OUTPUT_PATH, index=False)\n",
        "        print(f\"\\nSaved final data to: {OUTPUT_PATH}\")\n",
        "    else:\n",
        "        print(\"df_final is empty (or became empty after removals). Nothing to save or reorder.\")\n",
        "\n",
        "else: # This 'else' corresponds to 'if not df_engineered_features.empty:'\n",
        "    print(\"\\nFinal dataset could not be created as no engineered features were generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3RNYlX5GF4m",
        "outputId": "d0835a89-1381-498c-cc89-a0b589ca99bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 6. Merging All Data ---\n",
            "\n",
            "--- 7. Modifying Columns, Reordering, and Preparing for Save ---\n",
            "\n",
            "--- Head of Final Processed DataFrame ---\n",
            "  County Name State Name  Year   Latitude   Longitude  Hotspot  \\\n",
            "0         ada      idaho  2020  43.600699 -116.347853      NaN   \n",
            "1         ada      idaho  2021  43.600699 -116.347853      NaN   \n",
            "2         ada      idaho  2022  43.600699 -116.347853      NaN   \n",
            "3         ada      idaho  2023  43.600699 -116.347853      NaN   \n",
            "4         ada      idaho  2024  43.600699 -116.347853      NaN   \n",
            "\n",
            "   PM25_Annual_Mean  PM25_Annual_Max  PM25_Annual_StdDev  PM25_Annual_IQR  \\\n",
            "0          7.774561             49.2            8.302082            4.975   \n",
            "1          8.714545             82.6           10.878881            4.950   \n",
            "2          7.499138             42.5            6.156418            6.450   \n",
            "3          5.888073             25.9            4.447445            4.700   \n",
            "4          4.489286             41.9            5.764994            2.975   \n",
            "\n",
            "   ...  NO2_Seasonal_Avg_Summer  NO2_Seasonal_Avg_Winter  \\\n",
            "0  ...                 7.161993                11.941740   \n",
            "1  ...                 8.069964                11.321667   \n",
            "2  ...                 7.192321                14.194643   \n",
            "3  ...                 6.944117                11.851058   \n",
            "4  ...                 3.999167                11.614112   \n",
            "\n",
            "   SO2_Seasonal_Avg_Fall  SO2_Seasonal_Avg_Spring  SO2_Seasonal_Avg_Summer  \\\n",
            "0               0.223421                 0.254821                 0.177016   \n",
            "1               0.287917                 0.234048                 0.237121   \n",
            "2               0.193520                 0.226711                 0.419388   \n",
            "3               0.349371                 0.175495                 0.262302   \n",
            "4                    NaN                 0.268675                 0.230114   \n",
            "\n",
            "   SO2_Seasonal_Avg_Winter  CO_Seasonal_Avg_Fall  CO_Seasonal_Avg_Spring  \\\n",
            "0                 0.238719              0.255222                0.161795   \n",
            "1                 0.239123              0.244500                0.144686   \n",
            "2                 0.103321              0.247673                0.156461   \n",
            "3                 0.191211              0.208184                0.169992   \n",
            "4                 0.275000                   NaN                0.143499   \n",
            "\n",
            "   CO_Seasonal_Avg_Summer  CO_Seasonal_Avg_Winter  \n",
            "0                0.156476                0.211067  \n",
            "1                0.224145                0.212074  \n",
            "2                0.148341                0.224621  \n",
            "3                0.171614                0.225276  \n",
            "4                0.107709                0.208634  \n",
            "\n",
            "[5 rows x 51 columns]\n",
            "\n",
            "Shape of final DataFrame: (341, 51)\n",
            "Columns in final DataFrame: ['County Name', 'State Name', 'Year', 'Latitude', 'Longitude', 'Hotspot', 'PM25_Annual_Mean', 'PM25_Annual_Max', 'PM25_Annual_StdDev', 'PM25_Annual_IQR', 'O3_Annual_Mean', 'O3_Annual_Max', 'O3_Annual_StdDev', 'O3_Annual_IQR', 'NO2_Annual_Mean', 'NO2_Annual_Max', 'NO2_Annual_StdDev', 'NO2_Annual_IQR', 'SO2_Annual_Mean', 'SO2_Annual_Max', 'SO2_Annual_StdDev', 'SO2_Annual_IQR', 'CO_Annual_Mean', 'CO_Annual_Max', 'CO_Annual_StdDev', 'CO_Annual_IQR', 'PM25_Days_Above_Threshold', 'O3_Days_Below_Threshold', 'NO2_Days_Above_Threshold', 'SO2_Days_Above_Threshold', 'CO_Days_Above_Threshold', 'PM25_Seasonal_Avg_Fall', 'PM25_Seasonal_Avg_Spring', 'PM25_Seasonal_Avg_Summer', 'PM25_Seasonal_Avg_Winter', 'O3_Seasonal_Avg_Fall', 'O3_Seasonal_Avg_Spring', 'O3_Seasonal_Avg_Summer', 'O3_Seasonal_Avg_Winter', 'NO2_Seasonal_Avg_Fall', 'NO2_Seasonal_Avg_Spring', 'NO2_Seasonal_Avg_Summer', 'NO2_Seasonal_Avg_Winter', 'SO2_Seasonal_Avg_Fall', 'SO2_Seasonal_Avg_Spring', 'SO2_Seasonal_Avg_Summer', 'SO2_Seasonal_Avg_Winter', 'CO_Seasonal_Avg_Fall', 'CO_Seasonal_Avg_Spring', 'CO_Seasonal_Avg_Summer', 'CO_Seasonal_Avg_Winter']\n",
            "\n",
            "Saved final data to: /content/drive/My Drive/Big Data Project/Data/Processed/Semi_Supervised_Learning_Model.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Prediction using Graph Neural Network"
      ],
      "metadata": {
        "id": "-YsLZ-lyLAXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html\n",
        "!pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5WL6GF56M_ln",
        "outputId": "15dd3bfa-bafa-4519-eba6-42a870abe919"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt26cu124)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn as nn\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "oXIizfbeKa9u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "FILE_PATH = \"/content/drive/My Drive/Big Data Project/Data/Processed/Semi_Supervised_Learning_Model_Prediction.csv\"\n",
        "N_NEIGHBORS = 4\n",
        "HIDDEN_DIM_GNN = 64\n",
        "OUTPUT_DIM_GNN = 2    # Hotspot (1) / Not Hotspot (0)\n",
        "EPOCHS_GNN = 500      # Epochs per year; adjust as needed\n",
        "LEARNING_RATE_GNN = 0.01\n",
        "WEIGHT_DECAY_GNN = 5e-4\n",
        "YEARS_TO_PROCESS = [2020, 2021, 2022, 2023] # Years you want to model"
      ],
      "metadata": {
        "id": "q6LdTAaNKa63"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Loop for Processing Each Year ---\n",
        "all_yearly_results_df = pd.DataFrame()\n",
        "\n",
        "print(f\"--- Starting GNN Processing for Years: {YEARS_TO_PROCESS} ---\")\n",
        "\n",
        "try:\n",
        "    df_full_dataset = pd.read_csv(FILE_PATH)\n",
        "    print(f\"Full dataset loaded. Shape: {df_full_dataset.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at '{FILE_PATH}'. Please ensure it's uploaded or the path is correct.\")\n",
        "    # exit() # Consider exiting or handling as appropriate for your environment\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading the full dataset: {e}\")\n",
        "    # exit()\n",
        "\n",
        "# Check if df_full_dataset was loaded\n",
        "if 'df_full_dataset' not in locals() or df_full_dataset.empty:\n",
        "    print(\"Exiting script as full dataset could not be loaded.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "for current_year in YEARS_TO_PROCESS:\n",
        "    print(f\"\\n\\n{'='*15} Processing Year: {current_year} {'='*15}\")\n",
        "\n",
        "    try:\n",
        "        # --- 1. Data Loading and Preprocessing for the Current Year ---\n",
        "        print(f\"--- 1.1. Preprocessing Data for {current_year} ---\")\n",
        "        df_year = df_full_dataset[df_full_dataset['Year'] == current_year].copy()\n",
        "\n",
        "        if df_year.empty:\n",
        "            print(f\"No data found for {current_year}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        original_rows_for_year = len(df_year)\n",
        "\n",
        "        # Ensure 'State Name' exists\n",
        "        if 'State Name' not in df_year.columns:\n",
        "            print(f\"Warning: 'State Name' column missing for {current_year}. Skipping year.\")\n",
        "            continue\n",
        "        df_year.dropna(subset=['State Name'], inplace=True)\n",
        "        df_year = df_year[df_year['State Name'].astype(str).str.strip() != ''].copy()\n",
        "\n",
        "        # Use engineered coordinates, fallback if necessary, then drop NaNs\n",
        "        lat_col, lon_col = 'Eng_Latitude', 'Eng_Longitude'\n",
        "        if not (lat_col in df_year.columns and lon_col in df_year.columns):\n",
        "            print(f\"Warning: Engineered coordinates not found for {current_year}, trying 'Latitude', 'Longitude'.\")\n",
        "            lat_col, lon_col = 'Latitude', 'Longitude'\n",
        "        if not (lat_col in df_year.columns and lon_col in df_year.columns):\n",
        "            print(f\"ERROR: Essential Latitude/Longitude columns missing for {current_year}. Skipping year.\")\n",
        "            continue\n",
        "        df_year.dropna(subset=[lat_col, lon_col], inplace=True)\n",
        "\n",
        "        # Sort and create unique node IDs for this year's graph\n",
        "        df_year.sort_values(by=['State Name', 'County Name'], inplace=True)\n",
        "        df_year = df_year.drop_duplicates(subset=['State Name', 'County Name'], keep='first').reset_index(drop=True)\n",
        "        df_year['GlobalNodeID_Year'] = df_year.index # Node ID specific to this year's graph\n",
        "        n_total_nodes_year = len(df_year)\n",
        "\n",
        "        if n_total_nodes_year == 0:\n",
        "            print(f\"No data remaining after preprocessing for {current_year}. Skipping.\")\n",
        "            continue\n",
        "        print(f\"Processing {n_total_nodes_year} unique counties for {current_year} (Original rows for year: {original_rows_for_year}).\")\n",
        "\n",
        "        # Define features\n",
        "        start_feature_col = 'PM25_Annual_Mean'\n",
        "        all_cols = df_year.columns.tolist()\n",
        "        try:\n",
        "            start_index = all_cols.index(start_feature_col)\n",
        "            potential_features = all_cols[start_index:]\n",
        "            exclude_cols = [\n",
        "                'GlobalNodeID_Year', 'Hotspot', 'Age-adjusted rate per 10,000', 'Number of cases',\n",
        "                'Year', # Year column itself is not a feature for the snapshot\n",
        "                'County Name', 'State Name', 'Latitude', 'Longitude',\n",
        "                'Eng_Latitude', 'Eng_Longitude', 'CountyID' # Old ID if it exists\n",
        "            ]\n",
        "            features_cols = [col for col in potential_features if col not in exclude_cols]\n",
        "            if not features_cols: raise ValueError(\"No feature columns selected.\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error selecting features for {current_year}: {e}. Skipping year.\")\n",
        "            continue\n",
        "\n",
        "        # Impute and Scale features\n",
        "        node_features_raw = df_year[features_cols].copy()\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        node_features_imputed = imputer.fit_transform(node_features_raw)\n",
        "        scaler = StandardScaler()\n",
        "        node_features_scaled = scaler.fit_transform(node_features_imputed)\n",
        "        node_features_tensor = torch.tensor(node_features_scaled, dtype=torch.float)\n",
        "\n",
        "        # Prepare labels and train_mask for this year\n",
        "        labels_array = np.full(n_total_nodes_year, -1, dtype=np.int64)\n",
        "        df_year['State Name'] = df_year['State Name'].astype(str).str.strip().str.lower() # Standardize\n",
        "\n",
        "        # Use 'Hotspot' column directly (it should be 0/1 for CA, NaN for others from previous script)\n",
        "        # If 'Hotspot' column is missing, this will fail. Ensure it's in your CSV.\n",
        "        if 'Hotspot' not in df_year.columns:\n",
        "            print(f\"ERROR: 'Hotspot' column missing for {current_year}. Skipping year.\")\n",
        "            continue\n",
        "\n",
        "        california_mask_df = (df_year['State Name'] == 'california')\n",
        "        # Only use rows where 'Hotspot' is not NaN for labeling\n",
        "        labeled_ca_indices = df_year.loc[california_mask_df & df_year['Hotspot'].notna(), 'GlobalNodeID_Year'].values\n",
        "        labeled_ca_hotspot_values = df_year.loc[california_mask_df & df_year['Hotspot'].notna(), 'Hotspot'].values.astype(np.int64)\n",
        "\n",
        "        if len(labeled_ca_indices) > 0:\n",
        "            labels_array[labeled_ca_indices] = labeled_ca_hotspot_values\n",
        "\n",
        "        labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
        "        train_mask_tensor = (labels_tensor != -1) # Train on all nodes with a valid label (0 or 1)\n",
        "\n",
        "        print(f\"Node features shape for {current_year}: {node_features_tensor.shape}\")\n",
        "        print(f\"Number of labeled (training) nodes for {current_year}: {train_mask_tensor.sum().item()}\")\n",
        "\n",
        "        # --- 2. Graph Construction for Current Year ---\n",
        "        print(f\"--- 2.1. Building K-NN Graph for {current_year} ---\")\n",
        "        coordinates = df_year[[lat_col, lon_col]].values\n",
        "        adj_matrix = kneighbors_graph(coordinates, N_NEIGHBORS, mode='connectivity', include_self=False)\n",
        "        edge_index_sparse = adj_matrix.tocoo()\n",
        "        edge_index_tensor = torch.tensor([edge_index_sparse.row, edge_index_sparse.col], dtype=torch.long)\n",
        "        print(f\"Graph for {current_year} built with {edge_index_tensor.shape[1]} edges.\")\n",
        "\n",
        "        pyg_data = Data(x=node_features_tensor, edge_index=edge_index_tensor, y=labels_tensor, train_mask=train_mask_tensor)\n",
        "\n",
        "        # --- 4. GNN Model Definition (Instantiate new model for each year) ---\n",
        "        class GCN(nn.Module):\n",
        "            def __init__(self, num_node_features, hidden_dim, num_classes):\n",
        "                super(GCN, self).__init__()\n",
        "                self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
        "                self.conv2 = GCNConv(hidden_dim, num_classes)\n",
        "                self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "            def forward(self, data_obj):\n",
        "                x, edge_index = data_obj.x, data_obj.edge_index\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                x = self.dropout(x)\n",
        "                x = self.conv2(x, edge_index)\n",
        "                return F.log_softmax(x, dim=1)\n",
        "\n",
        "        gnn_model = GCN(num_node_features=pyg_data.x.shape[1],\n",
        "                        hidden_dim=HIDDEN_DIM_GNN,\n",
        "                        num_classes=OUTPUT_DIM_GNN)\n",
        "        optimizer = torch.optim.Adam(gnn_model.parameters(), lr=LEARNING_RATE_GNN, weight_decay=WEIGHT_DECAY_GNN)\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # --- 5. Training Loop for Current Year ---\n",
        "        if pyg_data.train_mask.sum().item() > 0: # Only train if there are labeled nodes\n",
        "            print(f\"--- 5.1. Starting Training for {current_year} ---\")\n",
        "            gnn_model.train()\n",
        "            for epoch in range(EPOCHS_GNN):\n",
        "                optimizer.zero_grad()\n",
        "                out = gnn_model(pyg_data)\n",
        "                loss = criterion(out[pyg_data.train_mask], pyg_data.y[pyg_data.train_mask])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if (epoch + 1) % 50 == 0:\n",
        "                    gnn_model.eval()\n",
        "                    pred_eval = gnn_model(pyg_data).argmax(dim=1)\n",
        "                    correct_train_eval = (pred_eval[pyg_data.train_mask] == pyg_data.y[pyg_data.train_mask]).sum()\n",
        "                    acc_train_eval = int(correct_train_eval) / int(pyg_data.train_mask.sum())\n",
        "                    print(f\"Year {current_year}, Epoch {epoch+1:03d}/{EPOCHS_GNN}, Loss: {loss.item():.4f}, Train Acc: {acc_train_eval:.4f}\")\n",
        "                    gnn_model.train()\n",
        "            print(f\"Training finished for {current_year}.\")\n",
        "        else:\n",
        "            print(f\"Skipping training for {current_year} - no labeled data found.\")\n",
        "\n",
        "        # --- 6. Prediction & Store Results for Current Year ---\n",
        "        print(f\"--- 6.1. Generating Predictions for {current_year} ---\")\n",
        "        gnn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            all_predictions_log_softmax = gnn_model(pyg_data)\n",
        "            all_predictions = all_predictions_log_softmax.argmax(dim=1)\n",
        "\n",
        "        df_year['Predicted_Hotspot_GNN'] = all_predictions.cpu().numpy()\n",
        "\n",
        "        # Evaluate on California (Labeled Data) for this year\n",
        "        ca_df_eval_year = df_year[df_year['State Name'] == 'california'].copy()\n",
        "        ca_df_eval_year = ca_df_eval_year[ca_df_eval_year['Hotspot'].notna()]\n",
        "        if not ca_df_eval_year.empty and pyg_data.train_mask.sum().item() > 0: # Only eval if trained\n",
        "            true_ca_labels_eval = ca_df_eval_year['Hotspot'].astype(int)\n",
        "            pred_ca_labels_eval = ca_df_eval_year['Predicted_Hotspot_GNN']\n",
        "            print(f\"\\n--- Evaluation on Labeled CA Counties for {current_year} ---\")\n",
        "            print(f\"Accuracy: {accuracy_score(true_ca_labels_eval, pred_ca_labels_eval):.4f}\")\n",
        "            print(classification_report(true_ca_labels_eval, pred_ca_labels_eval, zero_division=0, labels=[0,1]))\n",
        "\n",
        "        # Append current year's relevant results to the main collection DataFrame\n",
        "        # Ensure to select columns that exist in df_year\n",
        "        cols_to_append = ['Year', 'State Name', 'County Name', 'Hotspot', 'Predicted_Hotspot_GNN']\n",
        "        cols_present_in_df_year = [col for col in cols_to_append if col in df_year.columns]\n",
        "        all_yearly_results_df = pd.concat([all_yearly_results_df, df_year[cols_present_in_df_year]], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing year {current_year}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue # Move to the next year if an error occurs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFRKw7tDKa3_",
        "outputId": "a457ef75-728c-40f9-bcc0-6c4147aa068c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting GNN Processing for Years: [2020, 2021, 2022, 2023] ---\n",
            "ERROR: File not found at '/content/drive/My Drive/Big Data Project/Data/Processed/Semi_Supervised_Learning_Model_Prediction.csv'. Please ensure it's uploaded or the path is correct.\n",
            "\n",
            "\n",
            "=============== Processing Year: 2020 ===============\n",
            "--- 1.1. Preprocessing Data for 2020 ---\n",
            "Warning: Engineered coordinates not found for 2020, trying 'Latitude', 'Longitude'.\n",
            "Processing 65 unique counties for 2020 (Original rows for year: 65).\n",
            "Node features shape for 2020: torch.Size([65, 45])\n",
            "Number of labeled (training) nodes for 2020: 12\n",
            "--- 2.1. Building K-NN Graph for 2020 ---\n",
            "Graph for 2020 built with 260 edges.\n",
            "--- 5.1. Starting Training for 2020 ---\n",
            "Year 2020, Epoch 050/500, Loss: 0.4202, Train Acc: 0.7500\n",
            "Year 2020, Epoch 100/500, Loss: 0.2148, Train Acc: 0.9167\n",
            "Year 2020, Epoch 150/500, Loss: 0.2110, Train Acc: 1.0000\n",
            "Year 2020, Epoch 200/500, Loss: 0.2273, Train Acc: 0.9167\n",
            "Year 2020, Epoch 250/500, Loss: 0.1960, Train Acc: 1.0000\n",
            "Year 2020, Epoch 300/500, Loss: 0.2115, Train Acc: 0.9167\n",
            "Year 2020, Epoch 350/500, Loss: 0.1880, Train Acc: 1.0000\n",
            "Year 2020, Epoch 400/500, Loss: 0.1737, Train Acc: 1.0000\n",
            "Year 2020, Epoch 450/500, Loss: 0.2152, Train Acc: 1.0000\n",
            "Year 2020, Epoch 500/500, Loss: 0.2191, Train Acc: 1.0000\n",
            "Training finished for 2020.\n",
            "--- 6.1. Generating Predictions for 2020 ---\n",
            "\n",
            "--- Evaluation on Labeled CA Counties for 2020 ---\n",
            "Accuracy: 1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         4\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        12\n",
            "   macro avg       1.00      1.00      1.00        12\n",
            "weighted avg       1.00      1.00      1.00        12\n",
            "\n",
            "\n",
            "\n",
            "=============== Processing Year: 2021 ===============\n",
            "--- 1.1. Preprocessing Data for 2021 ---\n",
            "Warning: Engineered coordinates not found for 2021, trying 'Latitude', 'Longitude'.\n",
            "Processing 70 unique counties for 2021 (Original rows for year: 70).\n",
            "Node features shape for 2021: torch.Size([70, 45])\n",
            "Number of labeled (training) nodes for 2021: 12\n",
            "--- 2.1. Building K-NN Graph for 2021 ---\n",
            "Graph for 2021 built with 280 edges.\n",
            "--- 5.1. Starting Training for 2021 ---\n",
            "Year 2021, Epoch 050/500, Loss: 0.4376, Train Acc: 0.8333\n",
            "Year 2021, Epoch 100/500, Loss: 0.5879, Train Acc: 0.8333\n",
            "Year 2021, Epoch 150/500, Loss: 0.3687, Train Acc: 0.8333\n",
            "Year 2021, Epoch 200/500, Loss: 0.3706, Train Acc: 0.8333\n",
            "Year 2021, Epoch 250/500, Loss: 0.3330, Train Acc: 0.8333\n",
            "Year 2021, Epoch 300/500, Loss: 0.4907, Train Acc: 0.8333\n",
            "Year 2021, Epoch 350/500, Loss: 0.3897, Train Acc: 0.8333\n",
            "Year 2021, Epoch 400/500, Loss: 0.4048, Train Acc: 0.8333\n",
            "Year 2021, Epoch 450/500, Loss: 0.3019, Train Acc: 0.8333\n",
            "Year 2021, Epoch 500/500, Loss: 0.3011, Train Acc: 0.8333\n",
            "Training finished for 2021.\n",
            "--- 6.1. Generating Predictions for 2021 ---\n",
            "\n",
            "--- Evaluation on Labeled CA Counties for 2021 ---\n",
            "Accuracy: 0.8333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      1.00      0.88         7\n",
            "           1       1.00      0.60      0.75         5\n",
            "\n",
            "    accuracy                           0.83        12\n",
            "   macro avg       0.89      0.80      0.81        12\n",
            "weighted avg       0.87      0.83      0.82        12\n",
            "\n",
            "\n",
            "\n",
            "=============== Processing Year: 2022 ===============\n",
            "--- 1.1. Preprocessing Data for 2022 ---\n",
            "Warning: Engineered coordinates not found for 2022, trying 'Latitude', 'Longitude'.\n",
            "Processing 72 unique counties for 2022 (Original rows for year: 72).\n",
            "Node features shape for 2022: torch.Size([72, 45])\n",
            "Number of labeled (training) nodes for 2022: 12\n",
            "--- 2.1. Building K-NN Graph for 2022 ---\n",
            "Graph for 2022 built with 288 edges.\n",
            "--- 5.1. Starting Training for 2022 ---\n",
            "Year 2022, Epoch 050/500, Loss: 0.1446, Train Acc: 1.0000\n",
            "Year 2022, Epoch 100/500, Loss: 0.1013, Train Acc: 1.0000\n",
            "Year 2022, Epoch 150/500, Loss: 0.0675, Train Acc: 1.0000\n",
            "Year 2022, Epoch 200/500, Loss: 0.0162, Train Acc: 1.0000\n",
            "Year 2022, Epoch 250/500, Loss: 0.1760, Train Acc: 1.0000\n",
            "Year 2022, Epoch 300/500, Loss: 0.0107, Train Acc: 1.0000\n",
            "Year 2022, Epoch 350/500, Loss: 0.0406, Train Acc: 1.0000\n",
            "Year 2022, Epoch 400/500, Loss: 0.0117, Train Acc: 1.0000\n",
            "Year 2022, Epoch 450/500, Loss: 0.0114, Train Acc: 1.0000\n",
            "Year 2022, Epoch 500/500, Loss: 0.0083, Train Acc: 1.0000\n",
            "Training finished for 2022.\n",
            "--- 6.1. Generating Predictions for 2022 ---\n",
            "\n",
            "--- Evaluation on Labeled CA Counties for 2022 ---\n",
            "Accuracy: 1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         4\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        12\n",
            "   macro avg       1.00      1.00      1.00        12\n",
            "weighted avg       1.00      1.00      1.00        12\n",
            "\n",
            "\n",
            "\n",
            "=============== Processing Year: 2023 ===============\n",
            "--- 1.1. Preprocessing Data for 2023 ---\n",
            "Warning: Engineered coordinates not found for 2023, trying 'Latitude', 'Longitude'.\n",
            "Processing 70 unique counties for 2023 (Original rows for year: 70).\n",
            "Node features shape for 2023: torch.Size([70, 45])\n",
            "Number of labeled (training) nodes for 2023: 12\n",
            "--- 2.1. Building K-NN Graph for 2023 ---\n",
            "Graph for 2023 built with 280 edges.\n",
            "--- 5.1. Starting Training for 2023 ---\n",
            "Year 2023, Epoch 050/500, Loss: 0.2204, Train Acc: 1.0000\n",
            "Year 2023, Epoch 100/500, Loss: 0.0347, Train Acc: 1.0000\n",
            "Year 2023, Epoch 150/500, Loss: 0.0302, Train Acc: 1.0000\n",
            "Year 2023, Epoch 200/500, Loss: 0.0026, Train Acc: 1.0000\n",
            "Year 2023, Epoch 250/500, Loss: 0.0579, Train Acc: 1.0000\n",
            "Year 2023, Epoch 300/500, Loss: 0.0058, Train Acc: 1.0000\n",
            "Year 2023, Epoch 350/500, Loss: 0.0333, Train Acc: 1.0000\n",
            "Year 2023, Epoch 400/500, Loss: 0.0046, Train Acc: 1.0000\n",
            "Year 2023, Epoch 450/500, Loss: 0.0041, Train Acc: 1.0000\n",
            "Year 2023, Epoch 500/500, Loss: 0.0017, Train Acc: 1.0000\n",
            "Training finished for 2023.\n",
            "--- 6.1. Generating Predictions for 2023 ---\n",
            "\n",
            "--- Evaluation on Labeled CA Counties for 2023 ---\n",
            "Accuracy: 1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           1       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00        12\n",
            "   macro avg       1.00      1.00      1.00        12\n",
            "weighted avg       1.00      1.00      1.00        12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Final Output ---\n",
        "print(\"\\n\\n--- All Yearly Predictions Summary ---\")\n",
        "if not all_yearly_results_df.empty:\n",
        "    print(all_yearly_results_df.head())\n",
        "    print(f\"\\nShape of combined predictions: {all_yearly_results_df.shape}\")\n",
        "    print(\"\\nValue counts for GNN predictions across all years:\")\n",
        "    # Check if 'Predicted_Hotspot_GNN' exists before value_counts\n",
        "    if 'Predicted_Hotspot_GNN' in all_yearly_results_df.columns:\n",
        "        print(all_yearly_results_df['Predicted_Hotspot_GNN'].value_counts(dropna=False))\n",
        "    else:\n",
        "        print(\"'Predicted_Hotspot_GNN' column not found in the final results.\")\n",
        "\n",
        "    # Save the combined predictions\n",
        "    output_all_years_path = \"/content/drive/My Drive/Big Data Project/Data/Processed/all_years_gnn_predictions_semi_supervised.csv\"\n",
        "    all_yearly_results_df.to_csv(output_all_years_path, index=False)\n",
        "    print(f\"Saved all yearly predictions to {output_all_years_path}\")\n",
        "else:\n",
        "    print(\"No predictions were generated for any year.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4qE90rwKa1p",
        "outputId": "0cf668e1-681a-457f-adc1-fb59ae577a4b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- All Yearly Predictions Summary ---\n",
            "   Year  State Name County Name  Hotspot  Predicted_Hotspot_GNN\n",
            "0  2020     alabama   jefferson      NaN                      0\n",
            "1  2020     arizona    maricopa      NaN                      1\n",
            "2  2020     arizona        pima      NaN                      1\n",
            "3  2020    arkansas     pulaski      NaN                      1\n",
            "4  2020  california     alameda      1.0                      1\n",
            "\n",
            "Shape of combined predictions: (277, 5)\n",
            "\n",
            "Value counts for GNN predictions across all years:\n",
            "Predicted_Hotspot_GNN\n",
            "1    163\n",
            "0    114\n",
            "Name: count, dtype: int64\n",
            "Saved all yearly predictions to /content/drive/My Drive/Big Data Project/Data/Processed/all_years_gnn_predictions_semi_supervised.csv\n"
          ]
        }
      ]
    }
  ]
}